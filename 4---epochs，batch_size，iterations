深度学习中的epochs，batch_size，iterations详解---对这三个概念说的比较清楚


深度学习中的epochs，batch_size，iterations详解---对这三个概念说的比较清楚
2018.06.19 17:07 5320浏览
深度学习框架中涉及很多参数，如果一些基本的参数如果不了解，那么你去看任何一个深度学习框架是都会觉得很困难，下面介绍几个新手常问的几个参数。

batch
深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。

第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。

另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。

为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。

基本上现在的梯度下降都是基于mini-batch的，所以深度学习框架的函数中经常会出现batch_size，就是指这个。 
关于如何将训练样本转换从batch_size的格式可以参考训练样本的batch_size数据的准备。

iterations
iterations（迭代）：每一次迭代都是一次权重更新，每一次权重更新需要batch_size个数据进行Forward运算得到损失函数，再BP算法更新参数。1个iteration等于使用batchsize个样本训练一次。

epochs
epochs被定义为向前和向后传播中所有批次的单次训练迭代。这意味着1个周期是整个输入数据的单次向前和向后传递。简单说，epochs指的就是训练过程中数据将被“轮”多少次，就这样。

举个例子

训练集有1000个样本，batchsize=10，那么： 
训练完整个样本集需要： 
100次iteration，1次epoch。

具体的计算公式为： 
one epoch = numbers of iterations = N = 训练样本的数量/batch_size

注：

在LSTM中我们还会遇到一个seq_length,其实 
batch_size = num_steps * seq_length


#---------

Epochs
一个Epoch是整个数据集通过神经网络前后传递一次。

由于一个Epoch太大，无法立即输入计算机，所以我们将它分成几个较小的批。

我们为什么要使用不止一个Epoch？
我知道开始的时候是没什么意义的—通过一个神经网络传递整个数据集是不够的。我们需要将整个数据集多次传递给同一个神经网络。但请记住，我们使用的是有限的数据集，并使用梯度下降优化学习和图，这是一个迭代的过程。因此，遍历一次或一个epoch来更新权重是不够的。

一个epoch导致下图中的曲线拟合不足。



随着epochs数量的增加，神经网络中权重变化次数越多，曲线由欠拟合变为最优，再变为过拟合。

那么，正确的epochs的数量是什么呢？
不幸的是，这个问题没有正确的答案。对于不同的数据集，答案是不同的，但是你可以说，epochs的数量与你的数据的多样性有关……只是一个例子—你的数据集中只有黑猫还是更多样化的数据集？

Batch Size
单个批次中出现的训练样本总数。

注：Batch size和Batch size的数量是两个不同的东西。

什么是Batch？
就像我说的，你不能把整个数据集同时传递到神经网络。因此，你将dataset划分为许多的批次或者子集或者不同的部分。

就像你把一篇大文章分成多个子集/批次/子部分，如引言、梯度下降、Epoch、Batch size和Iterations ，这使得读者很容易阅读整篇文章并理解它。😄

Iterations
为了得到迭代数，你只需要知道乘法表或者有个计算器。😃

迭代是完成一个epoch所需的batches的数量。

注：batches的数量等于一个epoch中的迭代次数。

假设我们要用2000个训练样本。

我们可以将2000个样本的数据集分成500的batches，然后需要4次迭代才能完成一个epoch。

这里，Batch size是500，完成一个epoch的迭代数是4。
————————————————
版权声明：本文为CSDN博主「ronghuaiyang」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/u011984148/article/details/99439950




#-=-=-=-=-=-=-=-=-

BATCH SIZE
　　一个 batch 中的样本总数。记住：batch size 和 number of batches 是不同的。

　　BATCH 是什么？

　　在不能将数据一次性通过神经网络的时候，就需要将数据集分成几个 batch。

　　正如将这篇文章分成几个部分，如介绍、梯度下降、Epoch、Batch size 和迭代，从而使文章更容易阅读和理解。

　　迭代
　　理解迭代，只需要知道乘法表或者一个计算器就可以了。迭代是 batch 需要完成一个 epoch 的次数。记住：在一个 epoch 中，batch 数和迭代数是相等的。

　　比如对于一个有 2000 个训练样本的数据集。将 2000 个样本分成大小为 500 的 batch，那么完成一个 epoch 需要 4 个 iteration。
  
  
 #------------
 CIFAR10 数据集有 50000 张训练图片，10000 张测试图片。现在选择 Batch Size = 256 对模型进行训练。

每个 Epoch 要训练的图片数量： 
训练集具有的 Batch 个数： 
每个 Epoch 需要完成的 Batch 个数： 
每个 Epoch 具有的 Iteration 个数： 
每个 Epoch 中发生模型权重更新的次数： 
训练  代后，模型权重更新的次数： 
不同代的训练，其实用的是同一个训练集的数据。第  代和第  代虽然用的都是训练集的五万张图片，但是对模型的权重更新值却是完全不同的。因为不同代的模型处于代价函数空间上的不同位置，模型的训练代越靠后，越接近谷底，其代价越小。
————————————————
版权声明：本文为CSDN博主「刺客五六柒」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_39521554/article/details/84480429
  
  
  #------------
  深度学习中经常看到epoch、 iteration和batchsize，下面按自己的理解说说这三个的区别：
（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
（2）iteration：1个iteration等于使用batchsize个样本训练一次；
（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；
举个例子，训练集有1000个样本，batchsize=10，那么：
训练完整个样本集需要：
100次iteration，1次epoch。
关于batchsize可以看看这里。


  
